{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28274,"status":"ok","timestamp":1753703629795,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"ZZpJGJdHqQHn","outputId":"cf8c47b9-0d42-42f3-a59a-078c9500340d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Drive mounted successfully!\n"]}],"source":["# ===================================================================\n","#               COMPLETE DATA & MODEL SETUP SCRIPT\n","# ===================================================================\n","\n","# --- 1. SETUP AND IMPORTS ---\n","# -------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import torchaudio\n","import pandas as pd\n","import numpy as np\n","import os\n","from lxml import etree\n","from google.colab import drive\n","import torch.optim as optim\n","\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Drive mounted successfully!\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1519,"status":"ok","timestamp":1753703698468,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"SBizflU9rDvI","outputId":"ff706248-5ea7-486a-ff76-9756901ddfdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["test\t\t   train\t\t  train_speech\t    val_speech\n","test_metadata.csv  train_metadata.csv\t  val\n","test_speech\t   train_metadata.gsheet  val_metadata.csv\n"]}],"source":["!ls /content/drive/MyDrive/capstone_project/CROHME_2023"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1753703700515,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"njzlgdxWqV7Y","outputId":"5ec438db-8426-4c49-8905-d89307b0f3c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["\n","# --- 2. CONFIGURATION ---\n","# -------------------------------------------------------------------\n","# --- Paths ---\n","DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/capstone_project/CROHME_2023\"\n","TRAIN_CSV = os.path.join(DRIVE_PROJECT_ROOT, \"train_metadata.csv\")\n","VAL_CSV = os.path.join(DRIVE_PROJECT_ROOT, \"val_metadata.csv\")\n","TEST_CSV = os.path.join(DRIVE_PROJECT_ROOT, \"test_metadata.csv\")\n","\n","# --- Hyperparameters ---\n","BATCH_SIZE = 16\n","LEARNING_RATE = 0.0001\n","D_MODEL = 256\n","N_HEADS = 8\n","NUM_ENCODER_LAYERS = 4\n","NUM_DECODER_LAYERS = 4\n","\n","# --- Special Tokens ---\n","PAD_TOKEN = 0\n","SOS_TOKEN = 1 # Start of Sequence\n","EOS_TOKEN = 2 # End of Sequence\n","\n","# --- Device ---\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1753703704268,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"nu2tUqC8qYcV","outputId":"df3f944a-de1c-4400-cc61-49ff9778b25b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating vocabulary from training data...\n","Vocabulary created with 84 unique tokens.\n"]}],"source":["# --- 3. VOCABULARY CREATION ---\n","# -------------------------------------------------------------------\n","print(\"\\nCreating vocabulary from training data...\")\n","df_train = pd.read_csv(TRAIN_CSV)\n","all_chars = set()\n","for latex_str in df_train['latex_ground_truth']:\n","    all_chars.update(list(str(latex_str)))\n","\n","vocab = sorted(list(all_chars))\n","char_to_idx = {char: i+3 for i, char in enumerate(vocab)}\n","char_to_idx['<pad>'] = PAD_TOKEN\n","char_to_idx['<sos>'] = SOS_TOKEN\n","char_to_idx['<eos>'] = EOS_TOKEN\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","VOCAB_SIZE = len(char_to_idx)\n","print(f\"Vocabulary created with {VOCAB_SIZE} unique tokens.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1753703706234,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"BdhQBtEcqa0q","outputId":"3aec0e76-5f6f-4b12-90aa-ef5287ec9950"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Defining Dataset class and collation function.\n"]}],"source":["# --- 4. DATASET AND DATALOADER DEFINITION ---\n","# -------------------------------------------------------------------\n","\n","def parse_inkml(inkml_path):\n","    ns = {'inkml': 'http://www.w3.org/2003/InkML'}\n","    try:\n","        tree = etree.parse(inkml_path)\n","        traces = tree.findall('inkml:trace', namespaces=ns)\n","        all_points = []\n","        for stroke_id, trace in enumerate(traces):\n","            points = trace.text.strip().split(',')\n","            for point_str in points:\n","                coords = point_str.strip().split(' ')\n","                if len(coords) == 2:\n","                    x, y = map(float, coords)\n","                    all_points.append([x, y, stroke_id])\n","        return np.array(all_points, dtype=np.float32)\n","    except Exception as e:\n","        return np.zeros((1, 3), dtype=np.float32) # Return a placeholder on error\n","\n","def load_audio_spectrogram(audio_path):\n","    try:\n","        waveform, sample_rate = torchaudio.load(audio_path)\n","        transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=80)\n","        spectrogram = transform(waveform)\n","        return spectrogram.squeeze(0).T\n","    except Exception as e:\n","        # Print a warning for the problematic file and return None\n","        print(f\"--- WARNING: Failed to load {os.path.basename(audio_path)}. Error: {e} ---\")\n","        return None\n","\n","class MathDataset(Dataset):\n","    def __init__(self, csv_path, char_to_idx_map):\n","        self.df = pd.read_csv(csv_path).dropna()\n","        self.char_to_idx = char_to_idx_map\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        ink_data = parse_inkml(row['inkml_file_path'])\n","        spectrogram = load_audio_spectrogram(row['audio_file_path'])\n","\n","        # If loading failed, return None for all items\n","        if spectrogram is None:\n","            return None, None, None\n","\n","        label_str = str(row['latex_ground_truth'])\n","        label = [self.char_to_idx[char] for char in label_str]\n","        label = [SOS_TOKEN] + label + [EOS_TOKEN]\n","\n","        return torch.tensor(ink_data), spectrogram, torch.tensor(label)\n","\n","def collate_fn(batch):\n","    # Filter out samples that returned None\n","    batch = [item for item in batch if item[0] is not None and item[1] is not None]\n","\n","    # If the entire batch failed, return None\n","    if not batch:\n","        return None, None, None\n","\n","    # Proceed with the original collation logic\n","    ink_tensors, audio_tensors, label_tensors = zip(*batch)\n","    ink_padded = pad_sequence(ink_tensors, batch_first=True, padding_value=0)\n","    labels_padded = pad_sequence(label_tensors, batch_first=True, padding_value=PAD_TOKEN)\n","    audio_padded = pad_sequence(audio_tensors, batch_first=True, padding_value=0)\n","\n","    return ink_padded, audio_padded, labels_padded\n","\n","print(\"\\nDefining Dataset class and collation function.\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1753703768382,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"NC2pN7KUqd50","outputId":"fa5afd92-ea56-4a38-d3ee-1f1ae02bcc88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model component classes defined.\n"]}],"source":["# --- 5. MODEL ARCHITECTURE DEFINITION ---\n","# -------------------------------------------------------------------\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n","        return x\n","\n","class HandwritingEncoder(nn.Module):\n","    def __init__(self, input_size, d_model, num_heads, num_layers):\n","        super(HandwritingEncoder, self).__init__()\n","        self.d_model = d_model\n","        self.input_proj = nn.Linear(input_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, src):\n","        src = self.input_proj(src) * np.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src)\n","        return output.mean(dim=1)\n","\n","class AudioEncoder(nn.Module):\n","    def __init__(self, cnn_output_size, d_model, num_heads, num_layers):\n","        super(AudioEncoder, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU()\n","        )\n","        self.d_model = d_model\n","        self.input_proj = nn.Linear(cnn_output_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, src):\n","        src = src.unsqueeze(1)\n","        src = self.conv(src)\n","        batch_size, channels, time, features = src.shape\n","        src = src.permute(0, 2, 1, 3).reshape(batch_size, time, channels * features)\n","        src = self.input_proj(src) * np.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src)\n","        return output.mean(dim=1)\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_heads, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, tgt, memory):\n","        tgt_emb = self.embedding(tgt)\n","        tgt_pos = self.pos_encoder(tgt_emb)\n","        output = self.transformer_decoder(tgt_pos, memory)\n","        return self.fc_out(output)\n","\n","class MultimodalTransformer(nn.Module):\n","    def __init__(self, handwriting_encoder, audio_encoder, decoder, fusion_size, d_model):\n","        super(MultimodalTransformer, self).__init__()\n","        self.handwriting_encoder = handwriting_encoder\n","        self.audio_encoder = audio_encoder\n","        self.decoder = decoder\n","        self.fusion_fc = nn.Linear(fusion_size, d_model)\n","\n","    def forward(self, ink_data, audio_data, target_latex):\n","        ink_context = self.handwriting_encoder(ink_data)\n","        audio_context = self.audio_encoder(audio_data)\n","        fused_context = torch.cat((ink_context, audio_context), dim=1)\n","        memory = self.fusion_fc(fused_context).unsqueeze(1)\n","        predictions = self.decoder(target_latex, memory)\n","        return predictions\n","\n","print(\"Model component classes defined.\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5264,"status":"ok","timestamp":1753703777605,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"G_FzvnpdskB3","outputId":"f3c80198-f409-40bf-d141-4e718cb1ab79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original spectrogram shape: torch.Size([490, 80])\n","Shape after CNN layers: torch.Size([1, 64, 123, 20])\n","\n","ACTION: Your calculated CNN_OUTPUT_SIZE is: 1280\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (80) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n","  warnings.warn(\n"]}],"source":["# 1. Get one sample from your dataset\n","train_dataset = MathDataset(TRAIN_CSV, char_to_idx)\n","sample_ink, sample_audio, _ = train_dataset[0]\n","\n","print(f\"Original spectrogram shape: {sample_audio.shape}\") # Should be (Time, Features)\n","\n","# 2. Simulate the CNN pass to find the output shape\n","# Create a dummy encoder just for this test\n","temp_conv = nn.Sequential(\n","    nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n","    nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU()\n",")\n","\n","# Add a batch and channel dimension to the sample\n","sample_audio = sample_audio.unsqueeze(0).unsqueeze(0)\n","conv_output = temp_conv(sample_audio)\n","print(f\"Shape after CNN layers: {conv_output.shape}\")\n","\n","# 3. Calculate the correct size\n","batch_size, channels, time, features = conv_output.shape\n","calculated_size = channels * features\n","print(f\"\\nACTION: Your calculated CNN_OUTPUT_SIZE is: {calculated_size}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1753703781952,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"vS6VvqR7szjG","outputId":"e17ddbd2-78b3-4834-98af-dcd438fc90fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully loaded /content/drive/MyDrive/capstone_project/CROHME_2023/train_speech/form_026_E201.mp3\n","   Waveform shape: torch.Size([1, 97920])\n","   Spectrogram shape: torch.Size([490, 80])\n"]}],"source":["import torchaudio\n","\n","# Paste the full path to one of your .mp3 files here\n","test_audio_path = \"/content/drive/MyDrive/capstone_project/CROHME_2023/train_speech/form_026_E201.mp3\" # Example\n","\n","try:\n","    waveform, sr = torchaudio.load(test_audio_path)\n","    print(f\"✅ Successfully loaded {test_audio_path}\")\n","    print(f\"   Waveform shape: {waveform.shape}\")\n","\n","    # Also test the spectrogram transformation\n","    transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_mels=80)\n","    spectrogram = transform(waveform)\n","    print(f\"   Spectrogram shape: {spectrogram.squeeze(0).T.shape}\")\n","\n","except Exception as e:\n","    print(f\"❌ Failed to load audio file.\")\n","    print(f\"   Error: {e}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8674,"status":"ok","timestamp":1753703795574,"user":{"displayName":"Jabez Lawrence G","userId":"14092452470717432603"},"user_tz":-330},"id":"Hnk5R6bMqknf","outputId":"87681bb0-4f02-4203-b3a3-66c0f2e9b818"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model created and moved to cuda.\n","DataLoaders instantiated.\n","Loss function and optimizer instantiated.\n","\n","--- SETUP COMPLETE ---\n","You are now ready to write and run your training loop.\n"]}],"source":["\n","# --- 6. INSTANTIATION ---\n","# -------------------------------------------------------------------\n","\n","# !! ACTION REQUIRED: You must calculate the correct CNN_OUTPUT_SIZE !!\n","# This depends on your spectrogram dimensions after passing through the CNN.\n","# For an input spectrogram of (Time, 80), after two strides of 2, the feature dim becomes 80 / 4 = 20.\n","# So, CNN_OUTPUT_SIZE = 64 (channels) * 20 (features) = 1280.\n","# Please verify this calculation with a sample from your data.\n","CNN_OUTPUT_SIZE = 1280\n","\n","# Instantiate model components\n","handwriting_enc = HandwritingEncoder(input_size=3, d_model=D_MODEL, num_heads=N_HEADS, num_layers=NUM_ENCODER_LAYERS)\n","audio_enc = AudioEncoder(cnn_output_size=CNN_OUTPUT_SIZE, d_model=D_MODEL, num_heads=N_HEADS, num_layers=NUM_ENCODER_LAYERS)\n","decoder = TransformerDecoder(vocab_size=VOCAB_SIZE, d_model=D_MODEL, num_heads=N_HEADS, num_layers=NUM_DECODER_LAYERS)\n","\n","# Instantiate the main model\n","model = MultimodalTransformer(\n","    handwriting_encoder=handwriting_enc,\n","    audio_encoder=audio_enc,\n","    decoder=decoder,\n","    fusion_size=D_MODEL * 2,\n","    d_model=D_MODEL\n",").to(device)\n","\n","print(f\"\\nModel created and moved to {device}.\")\n","\n","# Instantiate DataLoaders\n","train_dataset = MathDataset(TRAIN_CSV, char_to_idx)\n","val_dataset = MathDataset(VAL_CSV, char_to_idx)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n","print(\"DataLoaders instantiated.\")\n","\n","# Instantiate Loss and Optimizer\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n","optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n","print(\"Loss function and optimizer instantiated.\")\n","\n","print(\"\\n--- SETUP COMPLETE ---\")\n","print(\"You are now ready to write and run your training loop.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wHey32M_vrQ9","outputId":"6651da3c-bd80-4da1-81c5-0fca6d9dfd26"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- STARTING MODEL TRAINING ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (80) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  ... Batch 100/775 processed\n","  ... Batch 200/775 processed\n","  ... Batch 300/775 processed\n","  ... Batch 400/775 processed\n"]}],"source":["import time\n","\n","# --- 7. TRAINING AND VALIDATION LOOP ---\n","# -------------------------------------------------------------------\n","def train_epoch(model, dataloader, optimizer, criterion, device):\n","    # Set the model to training mode. This enables layers like Dropout.\n","    model.train()\n","\n","    # Initialize a variable to accumulate the loss for the entire epoch.\n","    total_loss = 0\n","\n","    # Define how often to print a progress update (e.g., every 100 batches).\n","    print_every = 100\n","\n","    # Use enumerate to get both the index 'i' and the batch data.\n","    # Loop over each batch of data provided by the DataLoader.\n","    for i, (ink_batch, audio_batch, label_batch) in enumerate(dataloader):\n","\n","        # Skip this batch if data loading failed.\n","        if ink_batch is None:\n","            continue\n","\n","        # Move all data tensors to the selected device (e.g., 'cuda' for GPU).\n","        ink_batch = ink_batch.to(device)\n","        audio_batch = audio_batch.to(device)\n","        label_batch = label_batch.to(device)\n","\n","        # --- Prepare data for teacher forcing ---\n","        # The decoder input is the sequence except for the last token (<eos>).\n","        target_input = label_batch[:, :-1]\n","        # The expected output is the sequence except for the first token (<sos>).\n","        target_expected = label_batch[:, 1:]\n","\n","        # --- Forward Pass ---\n","        # Get the model's predictions for the given inputs.\n","        predictions = model(ink_batch, audio_batch, target_input)\n","\n","        # --- Loss Calculation ---\n","        # Reshape the predictions and target to be compatible with the loss function.\n","        predictions = predictions.reshape(-1, VOCAB_SIZE)\n","        target_expected = target_expected.reshape(-1)\n","\n","        # Calculate the loss between the model's predictions and the actual labels.\n","        loss = criterion(predictions, target_expected)\n","\n","        # --- Backward Pass and Optimization ---\n","        # Clear any gradients from the previous step.\n","        optimizer.zero_grad()\n","\n","        # Compute the gradients of the loss with respect to model parameters.\n","        loss.backward()\n","\n","        # Clip gradients to prevent them from exploding, which helps stabilize training.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        # Update the model's weights based on the computed gradients.\n","        optimizer.step()\n","\n","        # Add the loss of the current batch to the running total.\n","        total_loss += loss.item()\n","\n","        # --- Progress Indicator ---\n","        # Print a progress update every 'print_every' batches.\n","        if (i + 1) % print_every == 0:\n","            print(f'  ... Batch {i+1}/{len(dataloader)} processed')\n","\n","    # Return the average loss for the epoch.\n","    return total_loss / len(dataloader)\n","\n","def evaluate_epoch(model, dataloader, criterion, device):\n","    model.eval() # Set model to evaluation mode\n","    total_loss = 0\n","    with torch.no_grad():\n","        for ink_batch, audio_batch, label_batch in dataloader:\n","            if ink_batch is None:\n","                continue\n","\n","            # Move data to device\n","            ink_batch = ink_batch.to(device)\n","            audio_batch = audio_batch.to(device)\n","            label_batch = label_batch.to(device)\n","\n","            # Prepare target data\n","            target_input = label_batch[:, :-1]\n","            target_expected = label_batch[:, 1:]\n","\n","            # Forward pass\n","            predictions = model(ink_batch, audio_batch, target_input)\n","\n","            # Reshape for loss\n","            predictions = predictions.reshape(-1, VOCAB_SIZE)\n","            target_expected = target_expected.reshape(-1)\n","\n","            # Calculate loss\n","            loss = criterion(predictions, target_expected)\n","            total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)\n","\n","# --- Main Training Execution ---\n","NUM_EPOCHS = 25 # You can adjust this value\n","best_val_loss = float('inf')\n","model_save_path = os.path.join(DRIVE_PROJECT_ROOT, 'best_model.pth')\n","\n","print(\"\\n--- STARTING MODEL TRAINING ---\")\n","for epoch in range(NUM_EPOCHS):\n","    start_time = time.time()\n","\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n","    val_loss = evaluate_epoch(model, val_loader, criterion, device)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n","\n","    print(f\"Epoch {epoch+1:02}/{NUM_EPOCHS} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.4f} | Val. Loss: {val_loss:.4f}\")\n","\n","    # Save the best model based on validation loss\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), model_save_path)\n","        print(f\"\\t-> Model saved to {model_save_path}\")\n","\n","print(\"\\n--- TRAINING FINISHED ---\")"]},{"cell_type":"markdown","metadata":{"id":"mFmBMT0nuKlF"},"source":["After your model has finished training, the final phase is to evaluate its performance on the unseen test set and create a function to generate predictions.\n","\n","This will give you the final, objective results for your project.\n","\n","## Step 1: Load Your Best Model\n","\n","First, you need to create an instance of your model and load the saved weights from your best_model.pth file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcsHEsyPuBZj"},"outputs":[],"source":["# Create a new instance of your model\n","model_instance = MultimodalTransformer(\n","    handwriting_encoder=handwriting_enc,\n","    audio_encoder=audio_enc,\n","    decoder=decoder,\n","    fusion_size=D_MODEL * 2,\n","    d_model=D_MODEL\n",").to(device)\n","\n","# Load the saved weights\n","model_path = os.path.join(DRIVE_PROJECT_ROOT, 'best_model.pth')\n","model_instance.load_state_dict(torch.load(model_path))\n","print(\"Best model loaded successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"5VMJrdIWuO3Y"},"source":["## Step 2: Implement an Inference Function\n","\n","During training, you used \"teacher forcing\" (feeding the correct sequence to the decoder). For testing, the model must generate the output on its own. You need an inference function that uses a greedy decoding strategy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK_Cv5VnuCfY"},"outputs":[],"source":["def predict(model, ink_tensor, audio_tensor, max_length=150):\n","    model.eval()\n","    with torch.no_grad():\n","        # Move inputs to device and add a batch dimension\n","        ink_tensor = ink_tensor.unsqueeze(0).to(device)\n","        audio_tensor = audio_tensor.unsqueeze(0).to(device)\n","\n","        # Get the context from the encoders\n","        ink_context = model.handwriting_encoder(ink_tensor)\n","        audio_context = model.audio_encoder(audio_tensor)\n","        fused_context = torch.cat((ink_context, audio_context), dim=1)\n","        memory = model.fusion_fc(fused_context).unsqueeze(1)\n","\n","        # Start the output sequence with the <sos> token\n","        output_sequence = [SOS_TOKEN]\n","\n","        for _ in range(max_length):\n","            # Convert the current output sequence to a tensor\n","            target_tensor = torch.LongTensor(output_sequence).unsqueeze(0).to(device)\n","\n","            # Get the model's prediction for the next token\n","            predictions = model.decoder(target_tensor, memory)\n","\n","            # Get the token with the highest probability (greedy choice)\n","            next_token = predictions.argmax(2)[:, -1].item()\n","\n","            # Append the token to the sequence\n","            output_sequence.append(next_token)\n","\n","            # If the end-of-sequence token is predicted, stop\n","            if next_token == EOS_TOKEN:\n","                break\n","\n","    # Convert the sequence of indices back to characters\n","    return \"\".join([idx_to_char[idx] for idx in output_sequence])"]},{"cell_type":"markdown","metadata":{"id":"9PaysX5FuSbe"},"source":["## Step 3: Evaluate on the Test Set\n","\n","Now, loop through your test_loader, use your predict function on each sample, and compare the model's prediction to the actual ground truth label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YZfd6aNuE3o"},"outputs":[],"source":["# You may need to install this library for calculating CER\n","!pip install python-Levenshtein\n","\n","from Levenshtein import distance as levenshtein_distance\n","\n","test_dataset = MathDataset(TEST_CSV, char_to_idx)\n","# Note: Use a smaller batch size for evaluation if needed\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","total_cer = 0\n","exact_matches = 0\n","results = []\n","\n","for ink_sample, audio_sample, label_sample in test_loader:\n","    # Get the raw tensors for prediction\n","    ink = ink_sample.squeeze(0)\n","    audio = audio_sample.squeeze(0)\n","\n","    # Generate the prediction\n","    prediction_str = predict(model_instance, ink, audio)\n","\n","    # Get the ground truth string\n","    true_label_str = \"\".join([idx_to_char[idx.item()] for idx in label_sample.squeeze(0) if idx != PAD_TOKEN])\n","\n","    # Store results for inspection\n","    results.append({'truth': true_label_str, 'prediction': prediction_str})\n","\n","    # Calculate metrics\n","    if prediction_str == true_label_str:\n","        exact_matches += 1\n","    total_cer += levenshtein_distance(prediction_str, true_label_str)\n","\n","print(\"\\n--- FINAL EVALUATION RESULTS ---\")\n","print(f\"Exact Match Accuracy: {exact_matches / len(test_dataset):.4f}\")\n","print(f\"Average Character Error Rate (CER): {total_cer / len(test_dataset):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"2HvYNPTWuZE4"},"source":["This final step provides the concrete performance metrics for your project, proving how well your model has learned to recognize mathematical expressions."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN4o2fLRnIWBy0Q+RqSfp1v"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}